# Notes on using hpc2 cluster in HKUST

__Feng Wang__

_Oct 24, 2016_

## hpc2 websites
http://itsc.ust.hk/services/academic-teaching-support/high-performance-computing/hpc2-cluster

The technique details and consultation should go to ITSC of HKUST. One should carefully read the general rules and the tutorial on using hpc2 cluster on the website.

The tutorial on SLURM and compilers can also be found on the website.

> To access the cluster, user has to login the login-node, hpc2.ust.hk, via Secure Shell (SSH) on campus. User can compile the program and application there and submit them to run in the compute nodes with the batch queuing system SLURM.

Partitions: http://itsc.ust.hk/services/academic-teaching-support/high-performance-computing/hpc2-cluster/resource-limits/

Our partition name: `ph`

	* 3 nodes, 24 CPUs per node
	* 72 maxCPUs per User
	* 72 maxJobs per User
	* 108 maxSubmit per User
	* 30 days max wall time

General partition: `general`

	* 1 maxJobs per group
	* 2 maxNodes per group
	* 3 submitJobs per group
	* 1 day max wall time

Disk quota: 2TB per group
check disk usage:
```bash
lfs quota -g <your_group> /home
```

> Warning: NO backup service on the cluster

## Operating system and installed software
* OS: CentOS 6.6 64-bit

* CUDA Toolkit v7.5

* GCC v4.9.2 (default v4.4.7, location `/opt/rh/devtoolset-3`). To access
	```bash
	source /usr/local/setup/gcc-g++-4.9.2.sh
	```

* GSL v2.2 (`/usr/local/gsl`)
	* You can run `/usr/local/gsl/bin/gsl-config` to identify the link flags of the software for your compilation. In case you use the dynamic library, set up the `LD_LIBRARY_PATH` appropriately when you run your application.

* MVAPICH2 v2.2rc1 (`/usr/local/mvapich2-2.2rc1-gcc/`)
	* To access, add the following in your `~/.bash_profile`, and relogin again to be effective.
	```bash
	source /usr/local/setup/mvapich2-2.2rc1-gcc.sh
	```
	* to run MPI application with SLURM, use `srun` as the process manager
	```bash
	srun --mpi=pmi2 ./your_application
	```

* Open MPI v2.0.0 (`/usr/local/openmpi-2.0.0`)
	* add the following in your ~/.bash_profile, and relogin again to be effective. 
	```bash
	source /usr/local/setup/openmpi-2.0.0.sh
	```

* Python v2.7.9 and v3.3.6 (default v2.6.6, `/opt/python`),
	* To access, add the following in your ~/.bash_profile, and relogin again to be effective.
	```bash
	source /usr/local/setup/python-27-33.sh
	```
	* NumPy v1.8.0 is available with Python v2.7.9

* R v3.3.1 (`/usr/local/R`)

* cmake v2.8.12.2

Ref: http://itsc.ust.hk/services/academic-teaching-support/high-performance-computing/hpc2-cluster/software/

## Software installation on hpc2
### Boost
(http://www.boost.org/) Provides a repository for free peer-reviewed portable C++ source libraries

0. Before build anything, setup the gcc and python by
```
source /usr/local/setup/gcc-g++-4.9.2.sh
source /usr/local/setup/python-27-33.sh
```

1. Download the latest version and extract the source files
```bash
wget https://sourceforge.net/projects/boost/files/boost/1.62.0/boost_1_62_0.tar.gz
tar xzvf boost_1_62_0.tar.gz
cd boost_1_62_0
```

2. Read `INSTALL`. The detail on installation is in `more/getting_started/unix-variants.rst`

3. Run `./bootstrap.sh --help` to see what we have

4. Configure the binaries installation
```
./bootstrap.sh --prefix=$HOME/local/ --show-libraries 
```

5. build binaries (take a long time, ~ 20min, 1145 targets, but need to have a look to figure out if any error or warning happens. Be patient, any error may harm the future usage.)
```
./b2
```
You should see `The Boost C++ Libraries were successfully built!` to make sure your building is successful.

6. install
```
./b2 install
```
will leave Boost binaries in the `lib/` subdirectory of your installation prefix. You will also find a copy of the Boost headers in the `include/` subdirectory of the installation prefix, so you can henceforth use that directory as an `#include` path in place of the Boost root directory.:

### Dynamomd
There are two possible ways to install `dynamomd` on hpc2.

#### Install a pre-build version
`dynamomd` provides several pre-build binary files for Linux, especially, it provides a `rpm` package (v1.6.4346) for CentOS 6.6 (http://dynamomd.org/index.php/download).

One can download and extract the binary files from rpm to use `dynamomd`. This is the simplest way to install `dynamomd` on hpc2.

#### Install from source code
Make sure that you are going to keep a look at the logs for each command.

1. Clone the source
```
git clone https://github.com/toastedcrumpets/DynamO
```

2. `cd DynamO` and have a look at `README.md`

3. Follows the instruction on installation in `README.md`
```
mkdir build-dir
cd build-dir
cmake ../ -DCMAKE_INSTALL_PREFIX:PATH=$HOME/local/
```
Generally, you will see errors on the configuration. If so, go back to check which part goes wrong. Here is the wrong part of my log:
```
-- Could NOT find PythonInterp: Found unsuitable version "2.6.6", but required is at least "2.7" (found /usr/bin/python2)
  The compiler /usr/bin/c++ has limited C++11 support.  You must install a
  more modern C++ compiler.
  Although the compiler advertises support for C++0x/C++11, this support is
  limited (e.g., gcc-4.7 or lower).  You can only compile a limited version
  of DynamO (simple potentials only).
-- Could NOT find Boost
-- libJudy header/library missing.
-- Could NOT find GLUT (missing:  GLUT_glut_LIBRARY GLUT_INCLUDE_DIR) 
-- Could NOT find GLEW (missing:  GLEW_INCLUDE_DIR GLEW_LIBRARY)
-- Could NOT find GLX (missing:  GLX_LIBRARIES)
--   package 'libavutil' not found
--   package 'libavcodec' not found
--   package 'gtkmm-2.4' not found
--   package 'cairomm-1.0' not found
-- Gtkmm library not found - visualiser will not be built.
-- GLUT/freeGLUT library not found - visualiser will not be built.
-- GLEW library not found - visualiser will not be built.
-- Cairomm library not found - visualiser will not be built.
-- GLX library not found - visualiser will not be built.
  Python 2.7 not found, cannot install all DynamO tools.
CMake Error: The following variables are used in this project, but they are set to NOTFOUND.
Please set them or make sure they are set and tested correctly in the CMake files:
Boost_INCLUDE_DIR
```
After reading all the log, one can see that, the configuration complains mainly three parts: (1) newer gcc and python are missing, (2) `boost` is missing, (3) visualiser cannot be build due to the lack of opengl, audio, video packages.

Issue (1) can be fixed by using gcc 4.9 and python 2.7, 
```
source /usr/local/setup/gcc-g++-4.9.2.sh
source /usr/local/setup/python-27-33.sh
```

Issue (3) can be totally ignored, since visualiser is useless on hpc2. It also saves time if not build this useless part.

We need take care of `boost` for `C++` vectors, matrices, and relevant operations. In fact, matlab has all `libboost` we need, but the header files seem to be missing. To install `boost` for cpp, please refer the section on [boost installation](#boost). Once installed, run
```
rm -rf *
export BOOST_ROOT=$HOME/local/
cmake ../ -DCMAKE_INSTALL_PREFIX:PATH=$HOME/local/
```

4. Compile the codes (~20min)
```
make
```

5. Install
```
make install
```

6. Test
```
make test
```
The output should be

```
      Start  1: magnet_threadpool_test
 1/28 Test  #1: magnet_threadpool_test ............   Passed   10.60 sec
      Start  2: magnet_cubic_quartic_test
 2/28 Test  #2: magnet_cubic_quartic_test .........   Passed    0.07 sec
      Start  3: magnet_vector_test
 3/28 Test  #3: magnet_vector_test ................   Passed    0.01 sec
      Start  4: magnet_quaternion_test
 4/28 Test  #4: magnet_quaternion_test ............   Passed    0.03 sec
      Start  5: magnet_dilate_test
 5/28 Test  #5: magnet_dilate_test ................   Passed    7.65 sec
      Start  6: magnet_splinetest
 6/28 Test  #6: magnet_splinetest .................   Passed    0.01 sec
      Start  7: magnet_plane_intersection
 7/28 Test  #7: magnet_plane_intersection .........   Passed    0.01 sec
      Start  8: magnet_triangle_intersection
 8/28 Test  #8: magnet_triangle_intersection ......   Passed    0.01 sec
      Start  9: magnet_intersection_genalg
 9/28 Test  #9: magnet_intersection_genalg ........   Passed    2.54 sec
      Start 10: magnet_offcenterspheres
10/28 Test #10: magnet_offcenterspheres ...........   Passed    9.94 sec
      Start 11: magnet_stack_vector_test
11/28 Test #11: magnet_stack_vector_test ..........   Passed    0.01 sec
      Start 12: dynamo_hardsphere_test
12/28 Test #12: dynamo_hardsphere_test ............   Passed   10.82 sec
      Start 13: dynamo_shearing_test
13/28 Test #13: dynamo_shearing_test ..............   Passed   56.71 sec
      Start 14: dynamo_binaryhardsphere_test
14/28 Test #14: dynamo_binaryhardsphere_test ......   Passed   74.89 sec
      Start 15: dynamo_squarewell_test
15/28 Test #15: dynamo_squarewell_test ............   Passed   35.14 sec
      Start 16: dynamo_2dstepped_potential_test
16/28 Test #16: dynamo_2dstepped_potential_test ...   Passed   71.93 sec
      Start 17: dynamo_infmass_spheres_test
17/28 Test #17: dynamo_infmass_spheres_test .......   Passed   18.16 sec
      Start 18: dynamo_lines_test
18/28 Test #18: dynamo_lines_test .................   Passed   57.67 sec
      Start 19: dynamo_static_spheres_test
19/28 Test #19: dynamo_static_spheres_test ........   Passed   45.11 sec
      Start 20: dynamo_gravityplate_test
20/28 Test #20: dynamo_gravityplate_test ..........   Passed    6.38 sec
      Start 21: dynamo_polymer_test
21/28 Test #21: dynamo_polymer_test ...............   Passed   21.33 sec
      Start 22: dynamo_swingspheres_test
22/28 Test #22: dynamo_swingspheres_test ..........   Passed    7.07 sec
      Start 23: dynamo_squarewellwall_test
23/28 Test #23: dynamo_squarewellwall_test ........   Passed    0.36 sec
      Start 24: dynamo_thermalisedwalls_test
24/28 Test #24: dynamo_thermalisedwalls_test ......   Passed   15.30 sec
      Start 25: dynamo_event_sorters_test
25/28 Test #25: dynamo_event_sorters_test .........   Passed    0.59 sec
      Start 26: dynamo_replica_exchange
26/28 Test #26: dynamo_replica_exchange ...........   Passed   78.38 sec
      Start 27: dynamo_multicanonical_cmap
27/28 Test #27: dynamo_multicanonical_cmap ........   Passed   13.45 sec
      Start 28: dynamo_dynatransport
28/28 Test #28: dynamo_dynatransport ..............   Passed   49.59 sec

100% tests passed, 0 tests failed out of 28

Total Test time (real) = 593.78 sec
```

The installed binary is in `$HOME/local/bin`. If you prefer to use it without specifying the path, add
```bash
source /usr/local/setup/gcc-g++-4.9.2.sh
source /usr/local/setup/python-27-33.sh

export PATH=$PATH:$HOME/local/bin
```
into ~/.bash_profile

> To run simulations, please use SLURM. https://itsc.ust.hk/services/academic-teaching-support/high-performance-computing/hpc2-cluster/jobs/
